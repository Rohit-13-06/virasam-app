import requests
import json

def get_ai_response(prompt, use_fast_model=True):
    """
    Sends the user's prompt to Ollama with dynamic model selection.
    
    Args:
        prompt (str): The user's question/prompt
        use_fast_model (bool): If True, uses llama3.2:1b (fast), 
                               If False, uses llama3:latest (comprehensive)
    
    Returns:
        str: AI generated response or error message
    """
    # Dynamic model selection
    model_name = "llama3.2:1b" if use_fast_model else "llama3:latest"
    timeout_val = 60 if use_fast_model else 180  # 1 min vs 3 min
    
    try:
        url = "http://localhost:11434/api/generate"
        data = {
            "model": model_name,
            "prompt": f"Answer this question about cultural heritage in a helpful and informative way: {prompt}",
            "stream": False,
            "options": {
                "temperature": 0.7,
                "top_p": 0.9
            }
        }
        
        # Use dynamic timeout based on model
        resp = requests.post(url, json=data, timeout=timeout_val)
        
        if resp.status_code == 200:
            result = resp.json()
            ai_response = result.get("response", "").strip()
            if ai_response:
                # Optional: Add model info to response for debugging
                model_info = f" (Generated by {model_name})" if use_fast_model else f" (Generated by comprehensive model)"
                return ai_response
            else:
                return "I couldn't generate a response for that question. Please try rephrasing."
        else:
            return f"AI service returned error (Status: {resp.status_code}). Please try again."
            
    except requests.exceptions.ConnectionError:
        return "Cannot connect to AI service. Please ensure Ollama is running."
    except requests.exceptions.Timeout:
        model_type = "fast" if use_fast_model else "comprehensive"
        return f"AI response timed out using {model_type} model. Try {'switching to fast model' if not use_fast_model else 'a simpler question'}."
    except requests.exceptions.RequestException:
        return "Network error occurred while contacting AI service. Please try again."
    except json.JSONDecodeError:
        return "Received invalid response from AI service. Please try again."
    except Exception as e:
        return "AI service temporarily unavailable. Please try again later."

# Convenience functions for specific use cases
def get_fast_ai_response(prompt):
    """Quick responses using the lightweight model"""
    return get_ai_response(prompt, use_fast_model=True)

def get_comprehensive_ai_response(prompt):
    """Detailed responses using the full model"""
    return get_ai_response(prompt, use_fast_model=False)

def get_smart_ai_response(prompt):
    """
    Automatically choose model based on prompt complexity
    """
    # Simple heuristic: use comprehensive model for longer/complex questions
    complex_keywords = ['explain', 'analyze', 'compare', 'describe in detail', 'history of', 'origins of']
    is_complex = any(keyword in prompt.lower() for keyword in complex_keywords) or len(prompt.split()) > 15
    
    use_fast = not is_complex
    return get_ai_response(prompt, use_fast_model=use_fast)
